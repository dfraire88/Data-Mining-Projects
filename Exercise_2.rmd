title: "Data Mining Exercise 2"
author: "David Fraire & Kylie Taylor"
date: "3/15/2019"
output: pdf_document
---

## Saratoga House Prices



```{r, include=FALSE}
library(tidyverse)
library(mosaic)
data(SaratogaHouses)
# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]
```

The output below reflects work we have done to determine which variables included in the Saratoga Houses data contained in the 'mosaic' package in R. The first model that we ran was the medium length model from class that is the model we are trying to preform better than. This model estimates house prices using the variables "lotSize", "age", "livingArea", "pctCollege", "bedrooms", "bathrooms", "fireplaces", "rooms", "heating", "fuel", and "centralAir". The factors found to be most significant in estimating price are "lotSize", "livingArea", "bedrooms", "bathrooms", "rooms", and "centralAir".

The output from the medium length model in class is below. We see that there is an $R^2$ = 0.55 and a RMSE of 66,767 (averaged over 1,000 sampled RMSE's).  


```{r, echo=FALSE}	
set.seed(123990)
# Fit to the training data
lm.med = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
stargazer::stargazer(lm.med, type = "text")
rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}
#averaging over train/test splits
rmse_vals = do(1000)*{
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  # Fit to the training data
  lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  yhat_test2 = predict(lm2, saratoga_test)
  rmse(saratoga_test$price, yhat_test2)
}
colMeans(rmse_vals)
```



In an attempt to find a way to out-preform the model above, we used the package 'leaps' to run a best subsets on Saratoga houses. The best subsets algorithm cycles through all variables of interest in a data frame and combines them into separate regressions, then (in this case) compares the $R^2$ values of the many linear models that were generated by the algorithm. Below is a plot of the variables that best subsets suggests with the corresponding adjusted $R^2$. We see that if we include all the variables, the adjusted $R^2$ is about the same as if we were to leave a few superfluous variables, like "fireplace". A plot of adjusted $R^2$ on the y-axis and variables on the x-axis is below.



```{r, echo=FALSE}
library(leaps)
regsubsets.out <-
    regsubsets(price ~ . - landValue,
               data = SaratogaHouses,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```



We created two models to compete with the medium model built in class, while also letting the best subsets inform us that there is no variable that should clearly be left out. Before running any models, we made the decision to remove the variables "landValue" from this analysis, as it is measuring almost the same thing as price. It would not be "fair" to include in any models, if our ultimate goal is to determine which factor explains house prices the best. 

The first model we ran includes all the variables. The second model we ran includes all variables except "fireplace" and "sewer". The third and final model we tested was the same as the second plus interaction terms of "bedrooms$*$bathrooms", "age$*$heating" and "age$*$lotSize".
After averaging over 500 sampled RMSE's from the three models, we found that the second and the third model preform the best with equal RMSE's of 64,485.69.
```{r, echo=FALSE}
set.seed(129)
rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}
rmse_vals2 = do(500)*{
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  lm.med = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  lm <- lm(price ~. -landValue , data = saratoga_train)
  lm1 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir, data = saratoga_train)
  lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = saratoga_train)
  yhat_testmed = predict(lm.med, saratoga_test)
  yhat_test = predict(lm, saratoga_test)
  yhat_test1 = predict(lm1, saratoga_test)
  yhat_test3 = predict(lm3, saratoga_test)
  c(rmse(saratoga_test$price, yhat_testmed), rmse(saratoga_test$price, yhat_test), rmse(saratoga_test$price, yhat_test1), rmse(saratoga_test$price, yhat_test1))
}

colMeans(rmse_vals2)

```
We made the conclusion that the third model is the best, because the $R^2$ of the third model is the highest out of the three with an adjusted $R^2$ of 0.563. Therefore, this is the model we will be using for our analysis.
The linear model reveals that "lotSize", "livingArea", "bedrooms", "bathrooms", "rooms", "heating: water/steam", "waterFront", "newConstruction", "airCentral", "age$*$heating:water/steam", and "age$*$lotSize" are significant in estimating the value of a house in Saratoga.
```{r}
lm.med <- lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
lm <- lm(price ~. -landValue , data = saratoga_train)
lm1 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir, data = saratoga_train)
lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = saratoga_train)
stargazer::stargazer(lm, lm1, lm3, type = "text")
```




After determining that the third model was the best, we ran a KNN regression on the same variables. In order to do this, we standardized the variables. This resulted in the RMSEs being on a much different scale (the z-scale to be exact) than the RMSEs from the linear models. We ran KNN's on 7 different K values; 5, 20, 50, 70, 100, 200, and 300. 

A KNN model with 5 nearest neighbors appears to have the smallest RMSE out of the 7 models tested. This is an interesting trade off, because a low K results in low variance, but high bias. 

One way to compare if the KNN model does better than the linear model is to standardize the variables in the linear models and compare the RMSE's between the two methods. 
The standardized RMSE of the chosen linear model is 0.6536, this is significantly bigger than the RMSE for a KNN with 5 nearest neighbors of 0.3692. For this reason, we know that KNN is better preforming and therefore should be used.  


```{r, echo=FALSE}
library(readr)
library(psycho)
library(tidyverse)
library(dplyr)
library(FNN)
SaratogaHouses$I1 <- SaratogaHouses$bedrooms*SaratogaHouses$bathrooms
SaratogaHouses$gas <- ifelse(SaratogaHouses$fuel == "gas", 1, 0)
SaratogaHouses$electric <- ifelse(SaratogaHouses$fuel == "electric", 1, 0)
SaratogaHouses$oil <- ifelse(SaratogaHouses$fuel == "oil", 1, 0)
SaratogaHouses$septic <- ifelse(SaratogaHouses$sewer == "septic", 1, 0)
SaratogaHouses$sewerpublic <- ifelse(SaratogaHouses$sewer == "public/commercial", 1, 0)
SaratogaHouses$nosewer <- ifelse(SaratogaHouses$sewer == "none", 1, 0)
SaratogaHouses$YESwaterfront <- ifelse(SaratogaHouses$waterfront == "Yes", 1, 0)
SaratogaHouses$NOwaterfront <- ifelse(SaratogaHouses$waterfront == "No", 1, 0)
SaratogaHouses$YESnewConstruct <- ifelse(SaratogaHouses$newConstruction == "Yes", 1, 0)
SaratogaHouses$NOTnewConstruct <- ifelse(SaratogaHouses$newConstruction == "No", 1, 0)
SaratogaHouses$YESCentralAir <- ifelse(SaratogaHouses$centralAir == "Yes", 1, 0)
SaratogaHouses$NOCentralAir <- ifelse(SaratogaHouses$centralAir == "No", 1, 0)
SaratogaHouses$heatingsteam <- ifelse(SaratogaHouses$heating == "hot water/steam", 1,0)
SaratogaHouses$ageheatingsteam <- SaratogaHouses$age*SaratogaHouses$heatingsteam
SaratogaHouses$agelotSize <- SaratogaHouses$age*SaratogaHouses$lotSize
SaratogaHouses$bedbath <- SaratogaHouses$bedrooms*SaratogaHouses$bathrooms
SH <- SaratogaHouses %>% psycho::standardize()
rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}
K = 100
rmsevals = do(K)*{
  n = nrow(SH)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  SH_train = SH[train_cases,]
  SH_test = SH[test_cases,]
  x_train = select(SH_train, c(-fireplaces, -sewer, -landValue, -heating, -fuel, -waterfront, -newConstruction, -centralAir))
  y_train = select(SH_train, price)
  x_test = select(SH_test, c(-fireplaces, -sewer, -landValue, -heating, -fuel, -waterfront, -newConstruction, -centralAir))
  y_test = select(SH_test, price)
  knn5 = knn.reg(train=x_train, test = x_test, y = y_train, k=5)
  ypred_knn5 = knn5$pred
  knn20 = knn.reg(train=x_train, test = x_test, y = y_train, k=20)
  ypred_knn20 = knn20$pred
  knn50 = knn.reg(train=x_train, test = x_test, y = y_train, k=50)
  ypred_knn50 = knn50$pred
  knn70 = knn.reg(train=x_train, test = x_test, y = y_train, k=70)
  ypred_knn70 = knn70$pred
  knn100 = knn.reg(train=x_train, test = x_test, y = y_train, k=100)
  ypred_knn100 = knn100$pred
  knn200 = knn.reg(train=x_train, test = x_test, y = y_train, k=200)
  ypred_knn200 = knn200$pred
  knn300 = knn.reg(train=x_train, test = x_test, y = y_train, k=300)
  ypred_knn300 = knn300$pred
  c(rmse(SH_test$price, ypred_knn5), rmse(SH_test$price, ypred_knn20), rmse(SH_test$price, ypred_knn50), rmse(SH_test$price, ypred_knn70), rmse(SH_test$price, ypred_knn100), rmse(SH_test$price, ypred_knn200), rmse(SH_test$price, ypred_knn300))
}
colMeans(rmsevals)
NN <- c(5, 20, 50, 70 ,100, 200, 300)
plot(colMeans(rmsevals) ~ NN, main = "RMSE vs number of neighbors", ylab="RMSE", xlab = "Number of Neighbors", type = 'l')
```

```{r}
rmse_vals.std = do(500)*{
  n = nrow(SH)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  SH_train = SH[train_cases,]
  SH_test = SH[test_cases,]
  lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = SH_train)
  yhat_test3 = predict(lm3, SH_test)
  rmse(SH_test$price, yhat_test3)
}
colMeans(rmse_vals.std)
```



To conclude what these models have told us, the price of a house is best determined by lot size, age, proximity to a college, size of living area, number of bedrooms, number of bathrooms, number of rooms, if there is water/steam heating, type of fuel to the house, if its waterfront, if its new construction, if there is central air, the number of bedrooms times the number of bathrooms, age times if there is water/steam heating, and age times the lot size. We suggest that the local taxing authority should use a KNN model in contrast to a linear model to predict the prices of homes in Saratoga, NY, as a KNN model has the lowest out of sample RMSEs. To be specific, our analysis found that a KNN with 5 nearest neighbors has the lowest RMSE out of all the KNN models we tested. 



##Hospital Audit


This problem asks us to examine the performance of radiologists at a hospital in Seattle, WA. The data contains 790 observations of patients with variables accounting for 5 different radiologists, whether the patient got cancer, and various other risk factors. The first step we took was to run some summary statistics on the data frame. 

We found that approximately 3.75\% of all patients were diagnosed with breast cancer, there was about a 15\% recall rate overall, about 17.63\% of all patients have family history of breast cancer, and about 4.8\% of all patients have breast cancer symptoms. We also identified that the most common age of patients was between 40 and 49 years old, and the most common type of breast density is density 3, or heterogeneously dense, across all patients in this study. 

```{r, include=FALSE}
library(tidyverse)
library(nnet)
library(stats)
library(dplyr)
#library(sqldf)
library(stargazer)
library(pander)
brca <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/brca.csv", header=TRUE)
```

```{r, echo=FALSE}
pander(summary(brca))
```



We then created a confusion matrix displaying the cancer outcomes and recall decisions of all radiologists. We found that, as a group, the radiologists accurately make recall decisions about their patients in 85.7% of the instances recorded. 

```{r, echo=FALSE}
confusion_in = table(recall = brca$recall, cancer = brca$cancer)
confusion_in
Accuracy <- sum(diag(confusion_in))/sum(confusion_in) #total accuracy
stargazer::stargazer(Accuracy, type= "text", title= "Accuracy")
```



Our ultimate goal is not to find the overall rates of these parameters, rather how the 5 radiologists compare across all the parameters listed above. In this problem, our variable of interest is "recall", since the radiologist has little control over whether their patient is diagnosed with cancer, they do have control over if the patient is recalled (to hopefully catch cancer early on). Essentially, we are trying to determine which radiologist is best at their job.

The plot below reveals conditional probabilities of recall for each radiologist, given if the patient is diagnosed with cancer or not.
We find that radiologists 89 and 95 have the highest rates of recall, given their patients got diagnosed with cancer, about 71.42\%. The radiologist with the lowest recall given their patients' positive diagnosis were from radiologists 13 and 66, with a true positive rate of 50\% (a coin flip).


```{r, echo=FALSE}
aggregate(list(Recall =brca$recall), list(Radiologist = brca$radiologist, Cancer = brca$cancer), mean)
```



Now to address the question at hand: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?

Our tactic is to create a logistic model with recall as our outcome variable. We use a logistic model because our outcome variable, recall, is binary. To account for patient risk factors, we included all the factors in the data set as regressors in our model. 
We created a second model that included an interaction term for the estimated recall rate of each radiologist, given that a patient has breast cancer. This coefficient along with the coefficient on the radiologist variable should give us a good idea of which radiologist out of the 5 is the most conservative.


Output from the two models is below. We see that most significant determinant in recall is if the patient has cancer (intuitively). We also see that the model without the interaction term has a lower AIC than the one with the interaction term.

```{r, echo=FALSE}
glm0 <- glm(recall ~ cancer, data = brca, family = binomial)
glm1 <- glm(recall ~ . , data = brca, family = binomial)
glm2 <- glm(recall ~ . + radiologist*cancer, data = brca, family = binomial)
stargazer(glm1, glm2,  type = "text")
```




To interpret the results from the glm, we take the exponent of all the coefficients of our model. The results reveal that radiologist 66 and radiologist 89 have 1.49 and 1.61 higher odds, respectively, of recalling patients than radiologist 13. Alternatively, we found that radiologist 34 and radiologist 95 have 0.58 and 0.96 higher odds, respectively, than radiologist 13 (low odds of recall! since odds < 1).  

This means that radiologists 66 and 89 are more clinically conservative than radiologist 13, 34 and 95 when recalling patients, since they have a higher probability of recalling patients, thus recalling more patients.

```{r, echo=FALSE}
pander::pander(exp(coef(glm1)))
```




The next question of interest is when the radiologist is at the clinic, should they be weighting some clinical risk factors more heavily than they currently are?

For this question we are also dealing with a classification model, with a binary outcome variable, which means we will be using a logistic regression. Instead of wondering how individual radiologists performed, we are wondering if patient risk factors play a stronger role in determining cancer than radiologists thought. To do this, we compared a model with cancer as the outcome variable and recall as the explanatory variable to a model with cancer as the outcome variable and recall and all other risk factors as the explanatory variables. To determine if the radiologists are neglecting to account for patient's risk factors, we look at the fits of the models and which models estimate cancer the best.

The models reveal that the model that only regresses cancer on recall has a lower AIC than the model with all the risk factors included. This means that radiologists recalls are doing a relatively better job at estimating cancer diagnosis, therefore the data suggests that they do not need to be weighting some clinical factors more than they currently are.



```{r, echo=FALSE}
glm <- glm(cancer ~ recall, data = brca, family = binomial)
glm2 <- glm(recall ~ . , data = brca, family = binomial)
glm3 <- glm(cancer ~ . , data = brca, family = binomial)
stargazer(glm2, glm, glm3, type = "text")
```



To summarize our findings, we identified that there are two radiologists that tend to be more conservative when recalling patients than other radiologists. To be exact, radiologists 66 and 89 have higher probabilities than radiologists 13, 34 and 95 of recalling patients. We also identified radiologist 89 to have on of the highest true positive rates of recall given cancer at a rate of approximately 71\%. If the data were to recommend a radiologist based solely off recall rates, it would recommend radiologist 89 to do the patients' screenings. 
After determining how the radiologist preformed among themselves, we looked into if the radiologists might be ignoring some helpful information given by patients' risk factors. It was determined that the radiologists actually did a better job of diagnosing breast cancer when not accounting patients' risk factors. This conclusion was made because none of the variables in the model accounting for risk factors displayed statistical significance in determining cancer diagnosis, and the AIC of the cancer on recall model had a much lower AIC score. This information reveals that the radiologist's process for making a recall decision might actually be better if they do not look at the patient's risk factors.
A possible reason for this is because a patient's risk factors may sway the radiologists decision to recall the patient, likely in the case that they will not recall them because their risks appear to be lower.




##Viral Articles

The data set used in the analysis describes ~39,640 observations of articles published on Mashable.com's website. In the original data set there are 37 predictor variables that describe each article. 

We added to the initial data set by generating dummy variables indicating the specific category of article into a single categorical variable column. We repeated this same transformation to indicate the day of the week each article was published.

To begin building our models, we decided to plot the data points in various visualizations to find any striking relationships across the data. Below are a few of the most interesting plots and our findings from them.


```{r, include=FALSE}
library(readr)
library(FNN)
library(tidyverse)
library(datasets)
library(dbplyr)
library(dplyr)
library(ggformula)
library(ggplot2)
library(ggstance)
library(graphics)
library(grDevices)
library(lattice)
library(markdown)
library(Matrix)
library(methods)
library(mosaic)
library(mosaicData)
library(pander)
library(RColorBrewer)
library(rmarkdown)
library(stringr)
library(tidyr)
library(tidyverse)
library(utils)
onlinenews <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv")
onlinenews$viral <- as.numeric(onlinenews$shares > 1400)
colnames(onlinenews)
ON <- (onlinenews)
ON$viral <- ifelse(ON$shares > 1400, 1, 0)
## Day of the Week split
ON$Monday <- ifelse(ON$weekday_is_monday== 1,1,0)
ON$Tuesday <- ifelse(ON$weekday_is_tuesday == 1, 2, 0)
ON$Wednesday <- ifelse(ON$weekday_is_wednesday == 1, 3, 0)
ON$Thursday <- ifelse(ON$weekday_is_thursday == 1, 4, 0)
ON$Friday <- ifelse(ON$weekday_is_friday == 1, 5, 0)
ON$Saturday <- ifelse(ON$weekday_is_saturday == 1, 6, 0)
ON$Sunday <- ifelse(ON$weekday_is_sunday == 1, 7, 0)
ON$weekday_num = ON$Monday + ON$Tuesday + ON$Wednesday + ON$Thursday + ON$Friday + ON$Saturday + ON$Sunday
ON$weekday = factor(ON$weekday_num, levels=1:7, 
                    labels=c("Monday","Tuesday","Wednesday", "Thursday","Friday", "Saturday", "Sunday"))
ON$Entertainment <- ifelse(ON$data_channel_is_entertainment== 1,1,0)
ON$Social <- ifelse(ON$data_channel_is_socmed == 1, 2, 0)
ON$World <- ifelse(ON$data_channel_is_world == 1, 3, 0)
ON$Lifestyle <- ifelse(ON$data_channel_is_lifestyle == 1, 4, 0)
ON$Business <- ifelse(ON$data_channel_is_bus == 1, 5, 0)
ON$Tech <- ifelse(ON$data_channel_is_tech == 1, 6, 0)
ON$Category_num = ON$Entertainment + ON$Social + ON$World + ON$Lifestyle + ON$Business + ON$Tech
ON$Category = factor(ON$Category_num, levels=0:6, 
                    labels=c("Misc","Entertainment","Social","World", "Lifestyle","Business", "Tech" ))
```  



The first plot we created was a simple bar chart of number of shares of articles published on certain days of the week, with the color denoting the category of article. 


```{r, echo=FALSE}
  # Number of shares seems to be higher on Saturday/Sunday, gives us indication to add weekday
ggplot(ON, aes(weekday))+
  geom_bar(aes(fill=Category)) ## Count of articles per day
```



Here we learned that our data set contained mostly articles that were published on Tuesday, Wednesday, or Thursday, and that were classified under the general category of World, followed by Technology.

Given this information, we decided to create a box plot to explore the relationship between the number of shares across the days of the week for each particular type of category.


```{r, echo=FALSE}
## PLOTS
ggplot(ON, aes(x=weekday, y=shares))+
  geom_boxplot(aes(fill=Category))+
  ylim(0,2000)
```



Through this plot we found that across the week, shares of articles from all different types of sources is higher on the weekend. This led us to include "is_weekend" in our model.

The next piece we were interested in was how many articles were actually published by each type of channel, or category of article. Below is a simple bar plot of number of articles by the different articles, where we find that "World" and "Tech" channels have published the most amount of articles.


```{r, echo=FALSE}
ggplot(ON, aes(Category))+
  geom_bar(aes(fill=Category))+
  labs(title="# of Articles published per Category",
       x="",
       y="Number of Articles")
```



Naturally following, we were interested in the distribution of those articles that were shared across all the channels. 

We created the violin plots below and found that articles in the data set that were categorized as Social Media tended to have higher numbers of shares. The next categories that tended to demonstrate higher number of shares were Lifestyle and Tech articles. 


```{r, echo=FALSE}
## Violin Plot of Shares per type of Article Category
ggplot(ON, aes(x=Category, y=shares))+
  geom_violin(aes(fill=Category))+
  labs(title="Distribution of shares per Article Category",
       y="Shares",
       x="")+
  ylim(0,6000) ## This tells us that social media tends to have the higher number of shares out of all the categories
  
```




Given the information we gained visualizations, we hand-built a simple linear model including a mix of variables that were used to predict number of shares. The variables we chose to include were the following: number of hyperlinks, whether or not the article was published on the weekend, global rate of negative words, whether the article was Business, self reference average shares, whether the article was entertainment, number of keywords, and average negative polarity. Our visualizations showed a trend of higher shares on the weekend and seems to be very significant in our model. The particular type of article is a very deterministic part of predicting if an article will go viral as certain types of categories have higher shares. We tested this model against a few other possible specifications and found that the model described above is the strongest.

To begin, we start with a null model, which makes the prediction that every single article is viral. With this model, we will correctly predict that an article will go viral with a 49.34% success rate.

Our goal with this study is to preform better than just simply guessing that the article always goes viral.
The first model we generated was a linear regression with shares as the outcome variable, and the factors listed above as the explanatory variables. 


# Testing Performance for best Linear Model (lm2)
```{r, include=FALSE}
set.seed(123456)
library(mosaic)
rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}
rmse.IN.lm<- do(500)*{  
  # Train/Test Splits
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  #in sample
  phat_train2 = predict(lm2, news_train) 
  yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
  rmse(ON$shares, phat_train2)
}
IN.lm<- do(500)*{  
  # Train/Test Splits
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  #in sample
  phat_train2 = predict(lm2, news_train) 
  yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
#in sample performance
  confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)
  sum(diag(confusion_in2))/sum(confusion_in2)
}
rmse.OUT.lm <- do(500)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  phat_test2 <- predict(lm2, news_test)
  yhat_test2 = ifelse(phat_test2 >= 1400,1,0)
  rmse(ON$shares, phat_test2)
}
OUT.lm <- do(500)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  ## Fitting model to training data
  lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
  phat_test2 <- predict(lm2, news_test)
  yhat_test2 = ifelse(phat_test2 >= 1400,1,0)
  rmse(ON$shares, yhat_test2)
#out of sample performance
  confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)
  sum(diag(confusion_out2))/sum(confusion_out2)
  
}
```



Next, we averaged over 500 train-test splits and found that the in sample performance of the model has an RMSE of 11,685, a true positive rate of 0.994, a false positive rate of 0.991, a success rate of 49.43\%. 

```{r, echo=FALSE}
colMeans(rmse.IN.lm)
confusion_in2
colMeans(IN.lm)
```



The out of sample performance had a RMSE of 11,686, true positive rate of 0.993, a false positive rate of 0.994, and the same success rate of 49.43\%.  Both having a lift of 0.09\% from the null. 


```{r, echo=FALSE}
colMeans(rmse.OUT.lm)
confusion_out2
colMeans(OUT.lm)
```


We can conclude that the linear model for shares does not preform much better than the null.



Our next portion of the assignment was to determine if thresholding the shares so that any articles with shares over 1,400 would be considered viral and then running our analysis would render better results. In order to model for the binomial outcome variable "Viral", we need to use a logistic model. We stuck with the same variables in our model so we can compare to our linear model's Performance equally. 

We proceeded to assess the performance of this model by averaging over 500 train/test splits.


# Question 3 Part II: Classification Model using GLM Binomial 
```{r, include=FALSE}
set.seed(976)
rmse.IN.glm <- do(100)*{
## Train/ Test split for Generalized model 
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
## In-Sample Performance 
  phat_gtrain2 = predict(glm2, news_train) 
  yhat_gtrain2 = ifelse(phat_gtrain2>0.5, 1,0)
  rmse(ON$viral, phat_gtrain2)
}
IN.glm <- do(500)*{
## Train/ Test split for Generalized model 
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
## In-Sample Performance 
  phat_gtrain2 = predict(glm2, news_train) 
  yhat_gtrain2 = ifelse(phat_gtrain2>0.5, 1,0)
  confusion_ing2 = table(y = news_train$viral, yhat = yhat_gtrain2)
  sum(diag(confusion_ing2))/sum(confusion_ing2)
}
## Out of sample
rmse.OUT.glm <- do(100)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
  phat_testg2 = predict(glm2, news_test)
  yhat_testg2 = ifelse(phat_testg2 > 0.5,1,0)
  rmse(ON$viral, phat_testg2)
}
## Out of sample
OUT.glm <- do(500)*{
  n = nrow(ON)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = ON[train_cases,]
  news_test = ON[test_cases,]
  glm2 = glm(viral ~ num_hrefs +is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train, family='binomial')
  phat_testg2 = predict(glm2, news_test)
  yhat_testg2 = ifelse(phat_testg2 > 0.5,1,0)
  confusion_outg2 = table(y = news_test$viral, yhat = yhat_testg2)
  sum(diag(confusion_outg2))/sum(confusion_outg2)
}
```


The logistic model has an in sample RMSE of 0.945, a true positive rate of 0.224, a false positive rate of 0.087, and a success rate of 0.5695, or 56.95\%.

```{r, echo=FALSE}
colMeans(rmse.IN.glm)
confusion_ing2
colMeans(IN.glm)
```

The logistic model has an out of sample RMSE of 0.946, a true positive rate of 0.218, a false positive rate of 0.077, and a success rate of 0.5694, or 56.94\%.

```{r, echo=FALSE}
colMeans(rmse.OUT.glm)
confusion_outg2
colMeans(OUT.glm)
```

This means that the logistic model has a lift of about 7.6\% from the null. This is much better than the linear model. The logistic model appears to be more conservative with handing out predictions of viral than the linear model of shares, and the results appear to be slightly better because of it. 

Based off of the data, the approach of thresholding first, then regress/classify second performs better than the alternative. This is likely because the linear model was likely being pulled and influenced by articles that had very extreme amounts of shares. Once we turned the variable shares into a binary variable, the magnitudes of those extreme observations of shares do not influence the model, since they now carry the same weight as an article that has barely over 1,400 shares. 

Overall, our ability to predict if an article goes viral is very weak, barely better than simply guessing with a 50\% chance at its best. We believe that this is due to the nature of why articles get shared and go viral. For an article to go viral is truly a random process, because at it's root, there is a person reading the article deciding if they like it enough to share it, which is inherently random.
